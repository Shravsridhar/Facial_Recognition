{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementation of One shot learning using OpenFace.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "9zyKp_dR_V6V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementation of One shot learning\n",
        "    Opencv is used for pre-processing the images prior training\n",
        "    Transfer learning is used to replicate the Openface model structure and pretrained weights are taken for this purpose\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "pAyAA029_V6c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  What is One shot learning?\n",
        "    One short learning is where only one image per person is stored in database, which is passed through the neural network model built and an embedding vector is generated for each person in DB.\n",
        "    New embedding is created for person and compared with the stored embedding through euclidean distance. If there exist similarities between two vectors, person is recognized with the respective name."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "6liQ_umO_V6h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.engine.topology import Layer\n",
        "from keras import backend as K\n",
        "import glob\n",
        "from imutils import paths\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "from keras.utils import CustomObjectScope\n",
        "import pygame\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import utils\n",
        "import pickle\n",
        "import model\n",
        "from model import Model_Architecture"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_qcCcw9kALfk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Model_Architecture()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8A6QOtEdAOBS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weights = utils.weights\n",
        "weights_dict = utils.load_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D78uy4VVAOav",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for name in weights:\n",
        "  if model.get_layer(name) != None:\n",
        "    model.get_layer(name).set_weights(weights_dict[name])\n",
        "  elif model.get_layer(name) != None:\n",
        "    model.get_layer(name).set_weights(weights_dict[name])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Twup-jZdAQt3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save model weights\n",
        "model.save('./weights1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jt4KCVjGAbE6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# to load save weights\n",
        "with CustomObjectScope({'tf': tf}):\n",
        "    model = load_model('./weights1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e5M2kAPW_V8b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Capturing Images at instance for training\n",
        "    This helps capture upto 10 frames of a person. Further, any one image could be kept for training in database"
      ]
    },
    {
      "metadata": {
        "id": "lHrBpydM_V8l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cam = cv2.VideoCapture(0)\n",
        "\n",
        "face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "count = 0\n",
        "while(True):\n",
        "    ret, img = cam.read()\n",
        "    #gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_detector.detectMultiScale(img, 1.3, 5)\n",
        "    for (x,y,w,h) in faces:\n",
        "        x1 = x\n",
        "        y1 = y\n",
        "        x2 = x+w\n",
        "        y2 = y+h\n",
        "        cv2.rectangle(img, (x1,y1), (x2,y2), (255,255,255), 2)     \n",
        "        count += 1\n",
        "        # Save the captured image into the datasets folder\n",
        "        cv2.imwrite('C://Users//Abhishek//Face_recognition//New_dataset//Sri//Srii_' + str(count) + \".jpg\", img[y1:y2,x1:x2])\n",
        "        cv2.imshow('image', img)\n",
        "    k = cv2.waitKey(200) & 0xff # Press 'ESC' for exiting video\n",
        "    if k == 27:\n",
        "        break\n",
        "    elif count >= 10: # Take 30 face sample and stop video\n",
        "         break\n",
        "    \n",
        "cam.release()\n",
        "cv2.destroyAllWindows()\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yrLyv2Mh_V8t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Create embeddings for images"
      ]
    },
    {
      "metadata": {
        "id": "A3RRIFlH_V8v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def image_to_embedding(image, model):\n",
        "    image = cv2.resize(image, (96, 96)) \n",
        "    img = image[...,::-1]\n",
        "    img = np.around(np.transpose(img, (0,1,2))/255.0, decimals=12)\n",
        "    x_train = np.array([img])\n",
        "    embedding = model.predict_on_batch(x_train)\n",
        "    return embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bu2V3Wvu_V89",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recognizing Faces\n",
        "    For recognizing new images in real-time, euclidean distance is calculated. \n",
        "    Euclidean distance finds distance of two vectors on an euclidean space.\n",
        "    Comparing the similarity of captured image embedding and stored train embeddings, the character's name is predicted where the similarity is more. \n",
        "    HAAR filter in opencv is used to create the bounding box across face during the process."
      ]
    },
    {
      "metadata": {
        "id": "gnVYk4TK_V9B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def recognize_face(face_image, input_embeddings, model):\n",
        "\n",
        "    embedding = image_to_embedding(face_image, model) #changed for testing\n",
        "    \n",
        "    minimum_distance = 200\n",
        "    name = None\n",
        "    \n",
        "    # Loop over  names and encodings.\n",
        "    for (input_name, input_embedding) in input_embeddings.items():\n",
        "        euclidean_distance = np.linalg.norm(embedding-input_embedding)\n",
        "        print('Euclidean distance from %s is %s' %(input_name, euclidean_distance))\n",
        "        if euclidean_distance < minimum_distance:\n",
        "            minimum_distance = euclidean_distance\n",
        "            name = input_name\n",
        "    if minimum_distance < 0.80:\n",
        "        return str(name)\n",
        "    else:\n",
        "        return str(\"Unknown\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rDhAfag2_V9N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def create_input_image_embeddings():\n",
        "    input_embeddings = {}\n",
        "    path =[]\n",
        "    path = list(paths.list_images('C://Users//Abhishek//Face_recognition//New_dataset'))\n",
        "    for(i, imagePath) in enumerate(path):\n",
        "        person_name = imagePath.split(os.path.sep)[-2]\n",
        "        image_file = cv2.imread(imagePath)\n",
        "        input_embeddings[person_name] = image_to_embedding(image_file, model)\n",
        "        \n",
        "    return input_embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIPdVpowCDlJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Additional feature added**\n",
        "\n",
        "Sound effects are added which could be used for security purpose.\n",
        "Incase of a person's face being deteced, notification plays saying access granted and in case of unknown person being detected, warning sound plays.\n"
      ]
    },
    {
      "metadata": {
        "id": "4zzCuw50DC-K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime as dt\n",
        "pygame.mixer.init()\n",
        "pygame.mixer.set_num_channels(8)\n",
        "voice = pygame.mixer.Channel(2)\n",
        "s1 = pygame.mixer.Sound(\"detected.wav\")\n",
        "s2 = pygame.mixer.Sound(\"warning.wav\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o592Nu0q_V9V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Real-time recognition of faces"
      ]
    },
    {
      "metadata": {
        "id": "zylwA9Na_V9Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def recognize_faces_in_cam(input_embeddings):\n",
        "    \n",
        "\n",
        "    cv2.namedWindow(\"Face Recognition\")\n",
        "    vc = cv2.VideoCapture(0)\n",
        "   \n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "    \n",
        "    \n",
        "    while vc.isOpened():\n",
        "        _, frame = vc.read()\n",
        "        img = frame\n",
        "        height, width, channels = frame.shape\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "        # Loop through all the faces detected \n",
        "        identities = []\n",
        "        for (x, y, w, h) in faces:\n",
        "            x1 = x\n",
        "            y1 = y\n",
        "            x2 = x+w\n",
        "            y2 = y+h\n",
        "            \n",
        "            face_image = frame[max(0, y1):min(height, y2), max(0, x1):min(width, x2)]    \n",
        "            identity = recognize_face(face_image, input_embeddings, model)\n",
        "            \n",
        "  \n",
        "            if identity is not None:\n",
        "                img = cv2.rectangle(frame,(x1, y1),(x2, y2),(255,255,255),2)\n",
        "                cv2.putText(img, str(identity), (x1+5,y1-5), font, 1, (255,255,255), 2)\n",
        "                if voice.get_busy()== False:\n",
        "                    if(cur_time - begin_time).seconds > 10:\n",
        "                        begin_time = dt.datetime.now()\n",
        "                        voice.play(s1)\n",
        "            else:\n",
        "                cv2.putText(img,\"Unknown\",(x1+5,y1-5), font, 1, (255,255,0), 1)\n",
        "                if voice.get_busy()== False:\n",
        "                    if(cur_time - begin_time).seconds > 3:\n",
        "                        begin_time = dt.datetime.now()\n",
        "                        voice.play(s2)\n",
        "        \n",
        "        key = cv2.waitKey(100)\n",
        "        cv2.imshow(\"Face Recognization\", img)\n",
        "\n",
        "        if key == 27: # exit on ESC\n",
        "            break\n",
        "    vc.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m9NeZLuV_V9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_embeddings = create_input_image_embeddings()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLQQ-BluDwpt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Input embeddings can also be saved to disk for future use"
      ]
    },
    {
      "metadata": {
        "id": "1cdoJQZ5DsxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('train_faces1.dat', 'wb') as f:\n",
        "    pickle.dump(input_embeddings, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p02lO5PT_V9p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "recognize_faces_in_cam(input_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IDvuQNORD246",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To load saved input embeddings for train data"
      ]
    },
    {
      "metadata": {
        "id": "12YBgESXD6vj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('train_faces1.dat', 'rb') as f:\n",
        "    train_encodings = pickle.load(f)\n",
        "\n",
        "# Extract list of names and the list of encodings\n",
        "face_names = list(train_encodings.keys())\n",
        "face_encodings = np.array(list(train_encodings.values()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}